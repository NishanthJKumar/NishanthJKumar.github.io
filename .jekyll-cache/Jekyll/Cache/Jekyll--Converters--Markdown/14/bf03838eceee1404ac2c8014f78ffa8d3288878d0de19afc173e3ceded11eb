I"ØY<p style="text-align: center;font-size:110%;padding-top:40px">Welcome to my Research page! Iâ€™m currently a 1st Year Ph.D. student with the <a href="https://lis.csail.mit.edu/">LIS Group</a> within <a href="https://www.csail.mit.edu/">MIT CSAIL</a>. Iâ€™m officially advised by <a href="https://www.csail.mit.edu/person/leslie-kaelbling">Leslie Kaelbling</a> and <a href="https://people.csail.mit.edu/tlp/">Tomas Lozano-Perez</a>, though I frequently collaborate with <a href="https://scholar.google.com/citations?user=4mVPFQ8AAAAJ&amp;hl=en">Dylan Hadfield-Menell</a>, <a href="http://web.mit.edu/cocosci/josh.html">Josh Tenenbaum</a>, and many other wonderful people within CSAILâ€™s <a href="https://ei.csail.mit.edu/">Embodied Intelligence Initiative</a>. Iâ€™m extremely grateful for support from the <a href="https://engineering.brown.edu/news/2021-03-29/nsf-graduate-research-award">NSF Graduate Research Fellowship</a>.</p>

<p style="text-align: center;font-size: 100%"><a href="/misc_files/Nishanth_Resume.pdf">Resume</a> | <a href="/misc_files/Nishanth_CV.pdf">CV</a> | <a href="https://scholar.google.com/citations?user=FE512o4AAAAJ&amp;hl=en">Google Scholar</a> | <a href="/misc_files/njk-bio.txt">Bio</a> | <a href="https://github.com/NishanthJKumar">GitHub</a> | <a href="https://twitter.com/nishanthkumar23">Twitter</a></p>

<h2 id="research-areas">Research Areas</h2>
<p>Iâ€™m broadly interested in enabling robots to operate robustly in long-horizon, multi-task settings so that they can accomplish tasks like multi-object manipulation, cooking, or even performing household chores. To this end, Iâ€™m interested in combining classical AI planning and reasoning approaches with modern machine learning techniques. My research draws on ideas from reinforcement learning, task and motion planning (TAMP), continual learning, and neurosymbolic AI.</p>

<h2 id="publications">Publications</h2>
<h3 id="conference-papers">Conference Papers</h3>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <!-- <tr bgcolor="#ffffd0"> -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src="/images/paper-images/active-learning.png" width="160" />
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <p style="font-family:'Lato',Verdana,Helvetica,sans-serif; font-size:14px;font-weight:700">
              Just Label What You Need: Fine-Grained Active Selection for Perception and Prediction through Partially Labeled Scenes
              </p>
              <strong>Nishanth Kumar*</strong>,
              <a href="http://www.seansegal.com/">Sean Segal*</a>,
              <a href="http://www.cs.toronto.edu/~sergio/">Sergio Casas</a>,
              <a href="https://www.cs.toronto.edu/~mren/">Mengye Ren</a>,
              <a href="http://www.cs.toronto.edu/~wangjk/">Jingkang Wang</a>,
              <a href="http://www.cs.toronto.edu/~urtasun/">Raquel Urtasun</a>
              <br />
							<em>Conference on Robot Learning (CoRL)</em> poster, 2021.
              <br />
              <a href="https://openreview.net/forum?id=xQ8rr3-zpiH">OpenReview</a>
              /
              <a href="https://arxiv.org/abs/2104.03956">arXiv</a>
              /
              <a href="https://openreview.net/attachment?id=xQ8rr3-zpiH&amp;name=poster">poster</a>
              <br />
              <p>Introduces fine-grained active selection via partial labeling for efficient labeling for perception and prediction. <br />
              [* denotes equal contribution. Work was done while at Uber ATG]
              </p>
            </td>
          </tr>
        </tbody>
</table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src="/images/paper-images/aosm-paper.png" width="160" />
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <p style="font-family:'Lato',Verdana,Helvetica,sans-serif; font-size:14px;font-weight:700">
              Building Plannable Representations with Mixed Reality
              </p>
              <a href="http://cs.brown.edu/people/er35/">Eric Rosen</a>,
              <strong>Nishanth Kumar</strong>,
              <a href="https://nakulgopalan.github.io/">Nakul Gopalan</a>,
              <a href="https://scholar.google.com/citations?user=k3Oh9D0AAAAJ&amp;hl=en">Daniel Ullman</a>,
              <a href="https://cs.brown.edu/people/gdk/">George Konidaris</a>,
              <a href="https://cs.brown.edu/people/stellex/">Stefanie Tellex</a>
              <br />
							<em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2020.
              <br />
              <a href="https://h2r.cs.brown.edu/wp-content/uploads/rosen20.pdf">paper</a>
              /
              <a href="https://www.youtube.com/watch?v=dcFod--RMSI">video</a>
              <br />
              <p>Introduces Action-Oriented Semantic Maps (AOSM's) and a system to specify these with mixed reality, which robots can use to perform a wide-variety of household tasks.
              </p>
            </td>
          </tr>
        </tbody>
</table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src="/images/paper-images/oopomcp.png" width="160" />
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <p style="font-family:'Lato',Verdana,Helvetica,sans-serif; font-size:14px;font-weight:700">
              Multi-Object Search using Object-Oriented POMDPs
              </p>
              <a href="https://scholar.google.com/citations?user=5v2t5L0AAAAJ&amp;hl=en">Arthur Wandzel</a>,
              <a href="https://sites.google.com/view/robots-oh/yoonseon-oh">Yoonseon Oh</a>,
              <a href="https://www.linkedin.com/in/michael-fishman-9a0a11160/">Michael Fishman</a>,
              <strong>Nishanth Kumar</strong>,
              <a href="https://www.khoury.northeastern.edu/people/lawson-wong/">Lawson L.S Wong</a>,
              <a href="https://cs.brown.edu/people/stellex/">Stefanie Tellex</a>
              <br />
							<em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2019.
              <br />
              <a href="https://h2r.cs.brown.edu/wp-content/uploads/wandzel19.pdf">paper</a>
              /
              <a href="https://www.youtube.com/watch?v=ssmez0rjF1Y">video</a>
              <br />
              <p>Introduces the Object-Oriented Partially Observable Monte-Carlo Planning (OO-POMCP) algorithm for efficiently solving Object-Oriented Partially Observable Markov Decision Processes (OO-POMDPs) and shows how this can enable a robot to efficiently find multiple objects in a home environment.
              </p>
            </td>
          </tr>
        </tbody>
</table>

<h3 id="workshop-papers-and-extended-abstracts">Workshop Papers and Extended Abstracts</h3>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src="/images/paper-images/taxi-scoping.png" width="160" />
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <p style="font-family:'Lato',Verdana,Helvetica,sans-serif; font-size:14px;font-weight:700">
              Task Scoping for Efficient Planning in Open Worlds
              </p>
              <strong>Nishanth Kumar*</strong>,
              <a href="https://www.linkedin.com/in/michael-fishman-9a0a11160/">Michael Fishman*</a>,
              <a href="https://scholar.google.com/citations?user=1HIu1uQAAAAJ&amp;hl=en">Natasha Danas</a>,
              <a href="https://www.littmania.com/">Michael Littman</a>,
              <a href="https://cs.brown.edu/people/stellex/">Stefanie Tellex</a>
              <a href="https://cs.brown.edu/people/gdk/">George Konidaris</a>              
              <br />
							<em>AAAI Conference on Artificial Intelligence, Student Workshop,</em>, 2020.
              <br />
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/7195">paper</a>
              <br />
              <p>
              Introduces high-level ideas for how large Markov Decision Processes (MPDs) might be efficiently pruned to include only states and actions relevant to a particular reward function. This paper is subsumed by our arxiv preprint on task scoping.
              <br />
              [* denotes equal contribution]
              </p>
            </td>
          </tr>
        </tbody>
</table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src="/images/paper-images/mr-pick-place.png" width="160" />
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <p style="font-family:'Lato',Verdana,Helvetica,sans-serif; font-size:14px;font-weight:700">
              Knowledge Acquisition for Robots through Mixed Reality Head-Mounted Displays
              </p>
              <strong>Nishanth Kumar*</strong>,
              <a href="http://cs.brown.edu/people/er35/">Eric Rosen*</a>,              
              <a href="https://cs.brown.edu/people/stellex/">Stefanie Tellex</a>
              <br />
							<em>The Second International Workshop on Virtual, Augmented and Mixed Reality for Human Robot Interaction</em>, 2019.
              <br />
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/7195">paper</a>
              <br />
              <p>
              Sketches high level ideas for how a mixed reality system might enable users to specify information for a robot to perform pick-place and other household tasks. This work is subsumed by our AOSM work.
              <br />
              [* denotes equal contribution]
              </p>
            </td>
          </tr>
        </tbody>
</table>

<h3 id="preprints">Preprints</h3>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr bgcolor="#ffffd0">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div>
            <img src="/images/paper-images/task-scoping-image.png" width="160" />
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <p style="font-family:'Lato',Verdana,Helvetica,sans-serif; font-size:14px;font-weight:700">
          Task Scoping: Building Goal-Specific Abstractions for Planning in Complex Domains
          </p>
          <strong>Nishanth Kumar*</strong>,
          <a href="https://www.linkedin.com/in/michael-fishman-9a0a11160/">Michael Fishman*</a>,
          <a href="https://scholar.google.com/citations?user=1HIu1uQAAAAJ&amp;hl=en">Natasha Danas</a>,
          <a href="https://www.littmania.com/">Michael Littman</a>,
          <a href="https://cs.brown.edu/people/stellex/">Stefanie Tellex</a>
          <a href="https://cs.brown.edu/people/gdk/">George Konidaris</a>
          <br />
          <em>arXiv</em>, 2020.
          <br />
          <a href="https://arxiv.org/pdf/2010.08869">arxiv</a>
          <br />
          <p>
          Introduces a method for how large classical planning problems can be efficiently pruned to exclude states and actions that are irrelevant to a particular goal so that agents can solve very large, 'open-scope' domains that are capable of supporting multiple goals.
          <br />
          [* denotes equal contribution]
          </p>
        </td>
      </tr>
    </tbody>
</table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div>
            <img src="/images/paper-images/parameterized-bc.png" width="160" />
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <p style="font-family:'Lato',Verdana,Helvetica,sans-serif; font-size:14px;font-weight:700">
          Learning Deep Parameterized Skills from Demonstration for Re-targetable Visuomotor Control
          </p>
          <strong>Nishanth Kumar*</strong>,
          <a href="https://jdchang1.github.io/">Jonathan Chang*</a>,
          <a href="https://scholar.google.com/citations?user=JsWiJPcAAAAJ&amp;hl=en">Sean Hastings</a>,
          <a href="https://skylion007.github.io/">Aaron Gokaslan</a>,
          <a href="https://www.merl.com/people/romeres">Diego Romeres</a>,
          <a href="https://www.merl.com/people/jha">Devesh Jha</a>,
          <a href="https://www.merl.com/people/nikovski">Daniel Nikovski</a>,
          <a href="https://cs.brown.edu/people/gdk/">George Konidaris</a>,
          <a href="https://cs.brown.edu/people/stellex/">Stefanie Tellex</a>
          <br />
          <em>arXiv</em>, 2020.
          <br />
          <a href="https://arxiv.org/abs/1910.10628">arxiv</a>
          <br />
          <p>
          Shows how the generalization capabilities of Behavior Cloning (BC) can be improved by learning a policy parameterized by some input that enables the agent to distinguish different goals (e.g. different buttons to press in a grid). Includes several exhaustive experiments in simulation and on two different robots.
          <br />
          [* denotes equal contribution. Work was done in collaboration with Mitsubishi Electric Research Laboratories]
          </p>
        </td>
      </tr>
    </tbody>
</table>

<h3 id="theses-and-misc-publications">Theses and Misc. Publications</h3>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div>
            <img src="/images/paper-images/undergrad-thesis-image.png" width="160" />
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <p style="font-family:'Lato',Verdana,Helvetica,sans-serif; font-size:14px;font-weight:700">
          You Only Need Whatâ€™s in Scope: Generating Task-Specific Abstractions for Efficient AI Planning
          </p>
          <strong>Nishanth Kumar</strong>
          <br />
          <em>Undergraduate Honors Thesis, Brown University</em>, 2021.
          <br />
          <a href="/misc_files/Nishanth_Undergrad_Honors_Thesis.pdf">paper</a>
          <br />
          <p>
          Presents a detailed, thesis-style description of my work on Task Scoping. This work is largely subsumed by our 'Task Scoping' preprint.
          <br />
          </p>
        </td>
      </tr>
    </tbody>
</table>

<h2 id="talks">Talks</h2>
<ul>
  <li><a href="https://drive.google.com/file/d/1eshvDUiBJJIbLqqRusdgCv5O7wK076f8/view">Task Scoping: Generating Task-Specific Abstractions for Planning</a>
<br /> <a href="https://lis.csail.mit.edu/">MIT LIS Group Meeting</a>. February 12, 2021.</li>
  <li><a href="http://nishanthjkumar.com/research/">What Iâ€™m working on now: Task Scoping and Parameterized Imitation Learning</a>
<br /> Intelligent Robot Lab meeting. November, 2019.</li>
  <li><a href="https://www.youtube.com/watch?v=OOAPni0ZUW8">Letâ€™s Talk about AI and Robotics</a>
<br /> I was interviewed about my work, experiences and advice on research for an episode of the <a href="https://www.youtube.com/channel/UC-qYS_P9NvSRoWYUsV2FmYQ">interSTEM</a> YouTube channel.</li>
  <li><a href="http://awards.cs.brown.edu/2019/08/13/undergrad-nishanth-kumar-wins-best-plenary-presentation-ilurs/">Action-Oriented Semantic Maps via Mixed Reality</a>
<br /> The Second Ivy-League Undergraduate Research Symposuim (ILURS). Best Plenary Presentation Award.
<br /> The University of Pennsylvania. April, 2019.</li>
  <li><a href="https://www.youtube.com/watch?v=FziEfTBAwvg">Building intelligent, collaborative robots</a>
<br /> Machine Intelligence Conference 2019
<br /> Boston University. September 2019.</li>
</ul>

<h2 id="awards">Awards</h2>
<ul>
  <li><a href="https://www.nsfgrfp.org/">NSF GRFP Fellow</a></li>
  <li><a href="https://grad.berkeley.edu/admissions/apply/fellowships-entering/">Berkeley Fellowship</a> (declined)</li>
  <li><a href="https://www.sigmaxi.org/">Sigma Xi Inductee</a></li>
  <li><a href="http://awards.cs.brown.edu/2020/02/04/bayazit-galgana-kumar-and-safranchik-win-cra-outstanding-undergraduate-researcher-honorable-mentions/">CRA Outstanding Undergraduate Research Award Finalist</a>, 2021</li>
  <li><a href="https://www.tbp.org/recruit/recruitHome.cfm">Tau Beta Pi Inductee</a></li>
  <li><a href="https://www.heidelberg-laureate-forum.org/">Heidelberg Laureate</a>, 2020</li>
  <li><a href="https://goldwater.scholarsapply.org/">Goldwater Scholarship</a>, 2020.</li>
  <li><a href="http://awards.cs.brown.edu/2020/02/04/bayazit-galgana-kumar-and-safranchik-win-cra-outstanding-undergraduate-researcher-honorable-mentions/">CRA Outstanding Undergraduate Research Award Honorable Mention</a>, 2020</li>
  <li><a href="https://www.brown.edu/academics/college/fellowships/utra/named">Karen T. Romer Undergraduate Teaching and Research Award</a></li>
  <li>Best Plenary Presentation, The Second Ivy League Undergraduate Research Symposium, 2019</li>
</ul>

<h2 id="industry-experience-and-research-collaborations">Industry Experience and Research Collaborations</h2>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div>
            <img src="/images/paper-images/undergrad-thesis-image.png" width="160" />
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <p style="font-family:'Lato',Verdana,Helvetica,sans-serif; font-size:14px;font-weight:700">
          <a href="https://www.vicarious.com/">Vicarious AI</a>
          </p>
          <em>Research Intern</em>, Summer 2021.
          <p>
          Worked with <a href="https://stanniszhou.github.io/">Stannis Zhou</a>, <a href="https://scholar.google.com/citations?user=0vHt-XYAAAAJ&amp;hl=en">Wolfgang Lehrach</a>, and <a href="https://www.linkedin.com/in/miguel-l%C3%A1zaro-gredilla-133759a">Miguel Lazaro-Gredilla</a> on developing <a href="https://pgmax.readthedocs.io/">PGMax</a> - an open-source framework for ML with PGM's.
          <br />
          </p>
        </td>
      </tr>
    </tbody>
</table>

<ul>
  <li><a href="https://www.vicarious.com/">Vicarious AI</a>, Union City, USA. <br />Worked with <a href="https://stanniszhou.github.io/">Stannis Zhou</a>, <a href="https://scholar.google.com/citations?user=0vHt-XYAAAAJ&amp;hl=en">Wolfgang Lehrach</a>, and <a href="https://www.linkedin.com/in/miguel-l%C3%A1zaro-gredilla-133759a">Miguel Lazaro-Gredilla</a> on developing <a href="https://pgmax.readthedocs.io/">PGMax</a> - an open-source framework for ML with PGMâ€™s.</li>
  <li><a href="https://www.merl.com/research/">MERL</a>, Cambridge, USA. <br />Worked with <a href="https://www.merl.com/people/romeres">Diego Romeres</a>, <a href="https://www.merl.com/people/jha">Devesh Jha</a> and <a href="https://www.merl.com/people/nikovski">Daniel Nikovski</a> on furthering Learning from Demonstration for industrial robots.</li>
  <li><a href="https://www.uber.com/us/en/atg/research-and-development/">Uber ATG Research</a>, Toronto, Canada.
<br />Worked with <a href="https://www.uber.com/us/en/atg/research-and-development/researchers/sean-segal/">Sean Segal</a>, <a href="https://www.uber.com/us/en/atg/research-and-development/researchers/sergio-casas/">Sergio Casas</a>, <a href="https://www.uber.com/us/en/atg/research-and-development/researchers/wenyuan-zeng/">Wenyuan Zeng</a>, <a href="http://www.cs.toronto.edu/~wangjk/">Jingkang Wang</a>, <a href="https://www.cs.toronto.edu/~mren/">Mengye Ren</a> and others on a project exploring Active Learning for Self-Driving Vehicles that lead to a paper (read it <a href="https://arxiv.org/abs/2104.03956">here</a>!). I had the honor of being advised by <a href="http://www.cs.toronto.edu/~urtasun/">Prof. Raquel Urtasun</a>.</li>
</ul>

<h2 id="teaching">Teaching</h2>
<ul>
  <li><strong>Head Teaching Assistant</strong>, <a href="http://cs.brown.edu/courses/cs2951f/">CSCI 2951-F: Learning and Sequential Decision Making</a>
<br /> Brown University, Fall 2019</li>
  <li><strong>Teaching Assistant</strong>, <a href="https://selfservice.brown.edu/ss/bwckctlg.p_disp_course_detail?cat_term_in=201610&amp;subj_code_in=ENGN&amp;crse_numb_in=0031">ENGN 0031: Honors Intro to Engineering</a>
<br /> Brown University School of Engineering, Fall 2018</li>
</ul>

<h2 id="selected-press-coverage">Selected Press Coverage</h2>
<ul>
  <li><a href="https://engineering.brown.edu/news/2021-03-29/nsf-graduate-research-award">Engineeringâ€™s Dwyer, Dastin-van Rijn and Kumar Selected as NSF Graduate Research Fellows</a></li>
  <li><a href="https://cs.brown.edu/news/2021/01/13/bawabe-kumar-sam-walker-receive-cra-undergraduate-outstanding-researcher-honors/">Bawabe, Kumar, Sam, And Walke Receive CRA Outstanding Undergraduate Researcher Honors</a></li>
  <li><a href="https://engineering.brown.edu/news/2020-04-15/student-awards">Nishanth Kumar named 2020 Barry M. Goldwater Scholar</a></li>
  <li><a href="https://www.indiawest.com/news/global_indian/2020-barry-goldwater-scholars-include-many-impressive-indian-american-researchers/article_38eaa6d6-88bd-11ea-842a-b3d73797025a.html">2020 Barry Goldwater Scholars Include Many Impressive Indian American Researchers </a></li>
  <li><a href="https://cs.brown.edu/news/2019/08/13/undergrad-nishanth-kumar-wins-best-plenary-presentation-ilurs/">Undergrad Nishanth Kumar Wins Best Plenary Presentation At ILURS</a></li>
  <li><a href="http://cs.brown.edu/news/2020/02/04/bayazit-galgana-kumar-and-safranchik-win-cra-outstanding-undergraduate-researcher-honorable-mentions/">Bayazit, Galgana, Kumar, And Safranchik Win CRA Outstanding Undergraduate Researcher Honorable Mentions </a></li>
</ul>
:ET